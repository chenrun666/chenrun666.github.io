<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mongoDB]]></title>
    <url>%2F2019%2F10%2F13%2FmongoDB%2F</url>
    <content type="text"><![CDATA[关系型和非关系型的介绍 关系型数据库很强大，但是它并不能很好的应付所有的应用场景。MySQL的扩展性差，大数据下IO压力大，表结构更改困难 非关系数据库，易扩展，大数据高性能，灵活的数据模型，高可用 易扩展：数据之间没有关系 大数据量，高性能：NoSQL数据库都具有非常高的读写性能，尤其在大数据量下，同样变现优秀。 灵活的数据模型：NoSQL无需事先为要存储的数据建立字段，随时可以存储自定义的数据格式。 MongoDB 的安装OSX MongoDBbrew安装1sudo brew install mongodb 安装支持TLS/SSL 1sudo brew install mongodb --with-openssl curl 安装 1234567891011# 进入 /usr/localcd /usr/local# 下载sudo curl -O https://fastdl.mongodb.org/osx/mongodb-osx-ssl-x86_64-4.0.9.tgz# 解压sudo tar -zxvf &lt;tgz name&gt;# 重命名sudo mv &lt;old package name&gt; &lt;new package name&gt; docker 安装1234567891011# 搜索镜像docker search mongo# 拉取官方镜像docker pull mogo# 运行容器docker run -p 27017:27017 -v $PWD/db:/data/db -d mongo --name mongo-01# 进入容器内的mongodocker exec -it &lt;contains name&gt; mongo admin mongodb 操作mongodb 基本使用 查看数据库 1show dbs 使用数据库 1use admin 删除数据库 1db.dropDatabase() 数据类型 Object ID: 文档ID String: 字符串,最常用,必须是有效的UTF-8 Boolean: 存储一个布尔值,true或者false Integer: 整数可以是32位或64位,这取决于服务器 Double: 存储浮点值 Arrays: 数组或列表,多个值存储到一个键 Object: 用于嵌入式文档,即一个值为一个文档 Null: 存储Null值 Timestamp: 时间戳,表示1970-1-1到现在的总秒数 Date: 存储当前日期或时间的UNIX时间格式 创建日期语句如下: new Date(‘2019-10-13’) 每个文档都有一个属性,为_id,保证每个文档的唯一性 可以自己设置_id插入文档,如果没有提供,那么mongoDB为每个文档提供一个独特的_id,类型为ObjectID ObjectID是一个12字节的16进制数: 前4字节为当前的时间戳 接下来3个字节的机器ID 接下来的2个字节中的MongoDB的服务进程ID 最后三个字节是简单的增量值 创建 创建数据库 1use &lt;db name&gt; 创建集合 向不存在的集合中第一次加入数据时，集合会被创建出来 123db.createCollection(name, options)db.createCollection("stu")db.createCollection("sub", &#123;capped:true, size: 10&#125;) 参数capped： 默认为false表示不设置上限参数size： 当capped值为true，需要指定此参数，表示上限大小，当文档达到上限时，会将之前的数据覆盖，单位为字节 插入文档 1db.&lt;collection name&gt;.insert(document) 实例1db.col.insert(&#123;name: "chen", age: 20&#125;) 查看 查看集合 1show collections 查看文档内容 1db.col.find().pretty() 更新 更新文档 123456789db.collection.update( &lt;query&gt;, &lt;&gt;, &#123; upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;) 1db.collection.save(document) query: update的查询条件,类似sql update查询内where后面的 update: update的对象和一些更新的操作符($, $inc…)等, 可以理解为sql update查询内的set upsert: 可选,这个参数的意思是,如果不存在update的记录,是否插入objnew,true为插入,默认是false,不插入 mutli: 可选,mongodb默认是false,只更新找到的第一个条记录,如果这个参数是true,就是按照天及哦按查出来的多条记录全部更新 writeConcern: 可选,抛出异常的级别 实例1db.col.update(&#123;name: 'chen'&#125;, &#123;$set: &#123;name: 'peng'&#125;&#125;) 1db.col.update(&#123;name: "chen"&#125;, &#123;$set: &#123;age: 25&#125;&#125;, &#123;multi: true&#125;) 删除 删除集合 1db.&lt;集合名称&gt;.drop() 删除文档 1234db.collection.remove( &lt;query&gt;, &lt;justOne&gt;) MongoDB2.6以后1234567db.collection.remove( &lt;query&gt;, &#123; justOne: &lt;boolean&gt;, writeConcern: &lt;document&gt; &#125;) query: (可选)删除的文档条件 justOne: (可选)如果没有true或1,则只删除一个文档,如果不设置该参数,或者默认值为false, 则删除所有匹配条件的文档 writeConcern: (可选)抛出异常的级别 实例1db.col.remove(&#123;title: "MongoDB 教程"&#125;) 删除所有匹配 1db.col.remove(&#123;title: "MongoDB 教程"&#125;, 1) 删除一个 1db.col.remove(&#123;&#125;) 删除所有数据 remove()方法并不会真正释放空间.需要执行db.repairDatabase()来回收磁盘空间 数据备份和恢复备份1mongodump -h dbhost -d dbname -o dbdirectory h: 服务器地址,也可以是指定端口号 d: 需要备份的数据库名称 o: 备份的数据库存放位置,此目录存放着备份出来的数据 1mongodump -h 192.168.1.10:27017 -d test1 -p ~/db/data 恢复1mongorestore -h dbhost -d dbname --dir dbdirectory h: 服务器地址 d: 需要恢复的数据库实例 dir: 备份数据所在的位置 1mongorestore -h 192.168.1.10:27017 -d test2 --dir ~/data/db 聚合]]></content>
      <categories>
        <category>MongoDB</category>
      </categories>
      <tags>
        <tag>MongoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-决策树]]></title>
    <url>%2F2019%2F10%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[决策树构造 优点 计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理相关特征数据 缺点 可能产生过度匹配问题 决策树的一般流程 收集数据： 可以使用任务方法 准备数据： 树构造算法只适用于标称型数据，因此数值型数据必须离散化 分析数据： 可以使用任务方法，构造树完成之后，我们应该检查图形是否符合预期 训练算法： 构造树的数据结构 测试算法： 使用经验树计算错误率 使用算法： 测步骤可以使用于任何监督学习算法，而使用决策树可以更好地理解数据内在含义]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-KNN]]></title>
    <url>%2F2019%2F10%2F09%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-KNN%2F</url>
    <content type="text"><![CDATA[线性回归123456789101112131415161718192021222324252627282930313233from sklearn import datasets, linear_model # 引用 sklearn库，主要是使用其中的线性回归模块# 创建数据集，将数据写入到numpy数组import numpy as npimport matplotlib.pyplot as pltdata = np.array([[152, 51], [156, 53], [160, 54], [164, 55], [168, 57], [172, 60], [176, 62], [180, 65], [184, 69], [188, 72]])# 打印数组的大小~print(data.shape)x, y = data[:, 0].reshape(-1, 1), data[:, 1]# TODO 1. 实例化一个线性回归的模型regr = linear_model.LinearRegression()# TODO 2. 在x,y上训练一个线性回归模型。 如果训练顺利，则regr会存储训练完成之后的结果模型regr.fit(x, y)# TODO 3. 画出身高与体重之间的关系plt.scatter(x, y, color='red')# 画出已训练好的线条# plt.plot(x, regr.predict(x), color='blue')# 画x,y轴的标题plt.xlabel('height (cm)')plt.ylabel('weight (kg)')plt.show() # 展示# 利用已经训练好的模型去预测身高为163的人的体重print("Standard weight for person with 163 is %.2f" % regr.predict([[163]])) KNN12345678910111213141516from sklearn import datasetsfrom sklearn.model_selection import train_test_splitfrom sklearn.neighbors import KNeighborsClassifierimport numpy as npiris = datasets.load_iris()x = iris.datay = iris.targetx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2003)clf = KNeighborsClassifier(n_neighbors=3)clf.fit(x_train, y_train)correct = np.count_nonzero((clf.predict(x_test) == y_test) == True)print("Accuracy is : %.3f" % (correct / len(x_test))) 实现KNN算法1234567891011121314151617181920212223242526272829303132333435363738394041from sklearn import datasetsfrom collections import Counterfrom sklearn.model_selection import train_test_splitimport numpy as npiris = datasets.load_iris()x = iris.datay = iris.targetx_train, x_test, y_train, y_test = train_test_split(x, y, random_state=2003)def euc_dis(instance1, instance2): """ 计算两个样本instance1和instance2之间的欧式距离 instance1: 第一个样本， array型 instance2: 第二个样本， array型 """ # TODO dist = np.sqrt(sum((instance1 - instance2) ** 2)) return distdef knn_classify(x, y, testInstance, k): """ 给定一个测试数据testInstance, 通过KNN算法来预测它的标签。 X: 训练数据的特征 y: 训练数据的标签 testInstance: 测试数据，这里假定一个测试数据 array型 k: 选择多少个neighbors? """ # TODO 返回testInstance的预测标签 = &#123;0,1,2&#125; distances = [euc_dis(a, testInstance) for a in x] kneighbors = np.argsort(distances)[:k] count = Counter(y[kneighbors]) return count.most_common()[0][0]# 预测结果。predictions = [knn_classify(x_train, y_train, data, 3) for data in x_test]correct = np.count_nonzero((predictions == y_test) == True)print("Accuracy is: %.3f" % (correct / len(x_test))) 选择合适的K值 为了选择合理的K， 首先需要理解K对模型的影响 为了理解K对模型的影响，首先先理解什么叫模型的决策边界 决策边界 例1: 大学里60分以上为及格，60分一下不及格。那么60就是决策边界 例2: 今年某高校引入35岁一下，具有海外研究经验3年以上的的学者 决策边界分为两大类 线性决策边界 非线性决策边界 模型的泛化能力 在新的环境中的适应能力 随着K值的增大，决策边界也变得更加平滑。决策边界的平滑也意味着模型的稳定性。但是稳定不代表，这个模型就会越准确 交叉验证 交叉验证是机器学习建模中非常重要的一步，也是大多数人所说的“调参”的过程 核心思想就是把一些可能的K逐个去尝试一遍，然后选出效果最好的K值 把训练数据进一步分成训练数据和验证集，选择在验证数据里最好的超参数组合]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kali-主动信息收集]]></title>
    <url>%2F2019%2F10%2F07%2Fkali-%E4%B8%BB%E5%8A%A8%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[主动信息收集 直接与目标系统交互通信 无法避免留下访问的痕迹 使用受控的第三方电脑进行探测 使用代理或已经被控制的主机 做好被封杀的准备 使用噪声迷惑目标，淹没真实的探测流量 扫描 发送不同的侦测，根据返回结果判断目标状态 发现 识别活着的主机 潜在的攻击目标 输出一个IP地址列表 2，3，4层发现 发现—-二层发现 数据链路层 ARP 优点： 扫描速度块，可靠 缺点： 不可路由 ARP协议 抓包 arping arping 1.1.1.1 -c 1 arping 1.1.1.1 -d arping 1.1.1.1 -c 1 | grep “bytes from” | cut -d “ “ -f 5 | cut -d “(“ -f 2 | cut -d “)” -f 1 nmap 1.1.1.1-254 -sn nmap -iL iplist.txt -sn netdiscover -]]></content>
      <categories>
        <category>kali</category>
      </categories>
      <tags>
        <tag>kali</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql45讲-事务隔离]]></title>
    <url>%2F2019%2F09%2F30%2Fmysql45%E8%AE%B2-%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB%2F</url>
    <content type="text"><![CDATA[一个事务要更新一行，如果刚好有另外 一个事务拥有这一行的行锁，它又不能这么超然了，会被锁住，进入等待状态。问题是，既然进 入了等待状态，那么等到这个事务自己获取到行锁要更新数据的时候，它读到的值又是什么呢？ 举一个例子 12345create table `t` ( `id` int(11) NOT NULL, `k` int(11) DEFAULT NULL, primary key (`id`)) engine=InnoDB; 事务A 事务B 事务C start transaction with consistent snapshot; start transaction with consistent snapshot; update t set k=k+1 where id=1; update t set k=k+1 where id=1;select k from t where id=1; select K from t where id=1;commit; commit; 这里需要注意 事务的启动时机 begin/start transaction 命令并不是一个事务的起点，在执行到他们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用 start transaction with consistent snapshot 在MySQL里，有两个“视图”的概念 一个是view。他是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是create view…，而它的查询方法与表一样 另一个是InnoDB在实现MVCC时用到的一致性读视图，即consistent read view,用于支持RC9(Read Committed, 读提交)和RR (Repeatable Read, 可重复读) 隔离级别的实现。 它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”。 每一个事务都会标记一个rowtrx_id, 按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这个事务执行期间，其他事务的更新对它不可见。 在实现上。InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间。 数组里面的事务ID的最小值记为低水位，当前系统里面创建过的事务IJ为最大值加1结尾高水位。 这个事务数组和高水位，就组成了当前事务的一致性视图 而数据版本的可见性规则，就是基于数据的rowtrx_id和这个一致性视图的对比结果得到的。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[kali-被动信息收集]]></title>
    <url>%2F2019%2F09%2F28%2Fkali-%E8%A2%AB%E5%8A%A8%E4%BF%A1%E6%81%AF%E6%94%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[被动信息收集 +充值 -支付 北京的电子商务公司 - 北京 intitle:电子商务 intext:法人 intext:电话 语法 作用 site:qq.com 限制搜索范围的域名 inurl:.php?id=999 搜索网页上包含的URL intext:”后台管理” filetype:doc site:qq.com 搜索文件的后缀或者扩展名 intitle:登陆 限制搜索的网页标题 link: 得到一个所有包含了某个指定URL的页面列表 示例 查找后台地址 inurl:login|admin|manage|member|admin_login|login_admin|system|login|user|main|cms 查找文本内容 site:域名 intext:管理|后台|登陆|用户名|密码|验证码|系统|账号|admin|login|sys|managetem|password|password|username 查找可注入点 site:域名 inurl:aspx|jsp|php|asp 查找上传漏洞 site: 域名 inurl:ewebeditor|editor|uploadfile|eweb|edit 找eweb编辑器 site:域名 inurl:ewebeditor|editor|uploadfile|eweb|edit 存在的数据库 site:域名 filetype:mdb|asp|# 查看脚本类型 site:域名 filetype:asp/aspx/php/jsp 迂回策略入侵 inurl:cms/data/templates/images/index/ GHDB[https://www.exploit-db.com/google-hacking-database] 用户信息 邮件，主机 theharvester -d sina.com -l 300 -b google 文件 metagoofil -d microsoft.com -t pdf -l 200 -o test -f 1.html tmux 使用技巧新建窗口1tmux new -s SEESION_NAME 新建一个会话窗口 1tmux ls 查看所有的会话窗口 1tmux detach 临时退出会话窗口 分屏操作 快捷键：ctrl+b, 放开后再按% 水平分屏 快捷键：ctrl+b, 放开后再按“ 垂直分屏 快捷键：ctrl+b, 放开后按o 切换光标 快捷键：ctrl+b, 放开后按s 切换tmux会话终端 快捷键：ctrl+b, 放开后按t 显示事件 其他快捷操作 快捷键：ctrl+b, 放开后按&amp; 或者 exit 关闭当前会话 快捷键：ctrl+b, 放开后按x 关闭当前的窗口 快捷键：ctrl+b, 放开后按c 在当前基础上再打开一个窗口 快捷键：ctrl+b, 放开后按d 暂时退出当前会话 快捷键：ctrl+b, 放开后按q 关闭所有分屏的窗口，即合并为一个窗口 meltago的使用 申请账号 使用 其他途径 社交网络 工商注册 新闻组/论坛 招聘网站 http://www.archive.org/web/web.php 个人专属的密码字典 换个人信息生成其专属的密码字典 CUPP - Common User Password Profiler git clone https://github.com/Mebus/cupp.git python cup.py -i METADATA Exif图片信息 Foca RECON-NG 全特性的web侦查框架 基于python开发 1recon-ng 启动recon-ng 1help 查看所有命令 12marketplace helpmarketplace refresh 刷新模块 1marketplace search 查看所有模块，或者搜索某一个模块 1marketplace install netcraft 安装某一个模块 1modules search 查询某一个模块 安装模块遇到依赖问题。recon/domains-contacts/metacrawler 。错误提示：Module ‘recon/domains-contacts/metacrawler’ disabled. Dependency required: ‘’PyPDF3’’ 1modules load &lt;module name&gt; 使用一个模块 1info 查看信息 1options set SOURCE sina.com 1run]]></content>
      <categories>
        <category>kali</category>
      </categories>
      <tags>
        <tag>kali</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql45讲-减少行锁对性能的影响]]></title>
    <url>%2F2019%2F09%2F28%2Fmysql45%E8%AE%B2-%E5%87%8F%E5%B0%91%E8%A1%8C%E9%94%81%E5%AF%B9%E6%80%A7%E8%83%BD%E7%9A%84%E5%BD%B1%E5%93%8D%2F</url>
    <content type="text"><![CDATA[前言MySQL 的行锁是在引擎层由各个引擎自己实现的。但并不是所有的引擎都支持行锁，比如 MyISAM 引擎就不支持行锁。不支持行锁意味着并发控制只能使用表锁，对于这种引擎的表，同 一张表上任何时刻只能有一个更新在执行，这就会影响到业务并发度。InnoDB 是支持行锁的， 这也是MyISAM被InnoDB替代的重要原因之一。 行锁就是针对数据表中行记录的锁。这很好理解，比如事务A更新了一行，而这时候 事务B也要更新同一行，则必须等事务A的操作完成后才能进行更新。 当然，数据库中还有一些没那么一目了然的概念和设计，这些概念如果理解和使用不当，容易导 致程序出现非预期行为，比如两阶段锁。 两阶段锁说起 事务A 事务B begin; update t set k=k+1 where id=1; update t set k=k+2 where id=2; begin; update t set k=k+2 where id=1; commit; 这个问题的结论取决于事务A在执行完两条update语句后，持有哪些锁，以及在什么时候释放。 你可以验证一下：实际上事务B的update语句会被阻塞，直到事务A执行commit之后，事务B才能继续执行。 在 InnoDB事务中， 行锁是在需要的时候才加上的， 但并不是不需要了就立刻释 放， 而是要等到事务结束时才释放。 这个就是两阶段锁协议。 知道了这个设定，对我们使用事务有什么帮助呢？那就是，如果你的事务中需要锁多个行，要把 最可能造成锁冲突、最可能影响并发度的锁尽量往后放。我给你举个例子。 假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个 业务需要涉及到以下操作： 从顾客A账户余额中扣除电影票价； 给影院B的账户余额增加这张电影票价； 记录一条交易日志。 也就是说，要完成这个交易，我们需要update两条记录，并insert一条记录。当然，为了保证交易的原子性，我们要把这三个操作放在一个事务中。那么，你会怎样安排这三个语句在事务中的 顺序呢？ 试想如果同时有另外一个顾客C要在影院B买票，那么这两个事务冲突的部分就是语句2了。因为 它们要更新同一个影院账户的余额，需要修改同一行数据。 根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才 释放的。所以，如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额 这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。 死锁和死锁检测当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致 这几个线程都进入无限等待的状态，称为死锁。这里我用数据库中的行锁举个例子。 事务A 事务B begin; update t set k=k+1 where id=1; begin; update t set k=k+1 where id=2; update t set k=k+1 where id=2; update t set k=k+1 where id=1; 这时候，事务A在等待事务B释放id=2的行锁，而事务B在等待事务A释放id=1的行锁。 事务A和 事务B在互相等待对方的资源释放，就是进入了死锁状态。当出现死锁以后，有两种策略： 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数 innodb_lock_wait_timeout 来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事 务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。 InnoDB中，innodb_lock_wait_timeout的默认值是50s，意味着如果采用第一个策略，当出现 死锁以后，第一个被锁住的线程要过50s才会超时退出，然后其他线程才有可能继续执行。对于在线服务来说，这个等待时间往往是无法接受的。 但是，我们又不可能直接把这个时间设置成一个很小的值，比如1s。这样当出现死锁的时候，确 实很快就可以解开，但如果不是死锁，而是简单的锁等待呢？所以，超时时间设置太短的话，会 出现很多误伤。 所以，正常情况下我们还是要采用第二种策略，即：主动死锁检测，而且 innodb_deadlock_detect的默认值本身就是on。主动死锁检测在发生死锁的时候，是能够快速发 现并进行处理的，但是它也是有额外负担的。 每当一个事务被锁的时候，就要看看它所依赖的线程有没有被别人锁住，如此循环，最后判断是否出现了循环等待，也就是死锁。 每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是 O(n)的操作。假设有1000个并发线程要同时更新同一行，那么死锁检测操作就是100万这个量级 的。虽然最终检测的结果是没有死锁，但是这期间要消耗大量的CPU资源。因此，你就会看到 CPU利用率很高，但是每秒却执行不了几个事务。 根据上面的分析，我们来讨论一下，怎么解决由这种热点行更新导致的性能问题呢？ 问题的症结 在于，死锁检测要耗费大量的CPU资源。 如果你能确保这个业务一定不会出现死锁， 可以临时把死锁检 测关掉。 但是这种操作本身带有一定的风险，因为业务设计的时候一般不会把死锁当做一个严 重错误，毕竟出现死锁了，就回滚，然后通过业务重试一般就没问题了，这是业务无损的。而关 掉死锁检测意味着可能会出现大量的超时，这是业务有损的。 另一个思路是控制并发度。 根据上面的分析，你会发现如果并发能够控制住，比如同一行同时 最多只有10个线程在更新，那么死锁检测的成本很低，就不会出现这个问题。一个直接的想法 就是，在客户端做并发控制。但是，你会很快发现这个方法不太可行，因为客户端很多。我见过 一个应用，有600个客户端，这样即使每个客户端控制到只有5个并发线程，汇总到数据库服务 端以后，峰值并发数也可能要达到3000。 因此，这个并发控制要做在数据库服务端。如果你有中间件，可以考虑在中间件实现；如果你的 团队有能修改MySQL源码的人，也可以做在MySQL里面。基本思路就是，对于相同行的更新， 在进入引擎之前排队。这样在InnoDB内部就不会有大量的死锁检测工作了。 小结以两阶段协议为起点，和你一起讨论了在开发的时候如何安排正确的事务语句。这里的 原则/我给你的建议是：如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并 发度的锁的申请时机尽量往后放。 但是，调整语句顺序并不能完全避免死锁。所以我们引入了死锁和死锁检测的概念，以及提供了 三个方案，来减少死锁对数据库的影响。减少死锁的主要方向，就是控制访问相同资源的并发事 务量。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql45讲-全局锁和表锁]]></title>
    <url>%2F2019%2F09%2F28%2Fmysql45%E8%AE%B2-%E5%85%A8%E5%B1%80%E9%94%81%E5%92%8C%E8%A1%A8%E9%94%81%2F</url>
    <content type="text"><![CDATA[数据库锁设计的初衷是处理并发问题。作为多用户共享的资 源，当出现并发访问的时候，数据库需要合理地控制资源的访问规则。而锁就是用来实现这些访 问规则的重要数据结构。 根据加锁的范围， MySQL里面的锁大致可以分成全局锁、表级锁和行锁三类 。 全局锁全局锁就是对整个数据库实例加锁。MySQL 提供了一个全局读锁的方法，命令是 Flush tables with read lock(FTWRL)。 当你需要让整个库处于只读状态的时候，可以使用这个命令，之后其他线程的以下语句会被阻塞：数据更新语句（数据的增删改）、数据定义语句（包括 建表、修改表结构等）和更新类事务的提交语句。 全局锁的典型使用场景是，做全库逻辑备份。也就是把整库每个表都select出来存成文本在备份过程中整个库存完全处于只读状态 如果你在主库上备份，那么在备份期间都不能执行更新，业务基本就得停摆； 如果你在从库上备份，那么备份期间从库不能执行主库同步过来的binlog，会导致主从延迟。 不加锁的话，备份系统备份的得到的库不是一个逻辑时间点，这个视图是逻辑不一致的。 可以使用可重复读隔离级别 表级锁MySQL 里面表级别的锁有两种：一种是表锁，一种是元数据锁（meta data lock，MDL)。 读锁之间不互斥，因此你可以有多个线程同时对一张表增删改查。 读写锁之间、写锁之间是互斥的，用来保证变更表结构操作的安全性。因此，如果有两个线 程要同时给一个表加字段，其中一个要等另一个执行完才能开始执行。 12alter table tb1_name nowait add columnalter table tb1_name wait n add column 小结全局锁主要用在逻辑备份过程中。对于全部是InnoDB引擎的库，我建议你选择使用–singletransaction参数，对应用会更友好。 表锁一般是在数据库引擎不支持行锁的时候才会被用到的。如果你发现你的应用程序里有lock tables这样的语句，你需要追查一下，比较可能的情况是： 要么是你的系统现在还在用MyISAM这类不支持事务的引擎，那要安排升级换引擎； 要么是你的引擎升级了，但是代码还没升级。我见过这样的情况，最后业务开发就是把lock tables 和 unlock tables 改成 begin 和 commit，问题就解决了。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F09%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
